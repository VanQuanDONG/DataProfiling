{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "17f0aecf-35c2-491e-b2cf-29f181a7a455",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ID\\thomeaddress\\tuni\\n', '1\\tashleya\\tMac\\n', '2\\tcowper\\tUNSW']\n",
      "\\t\n",
      "\\n\n",
      "Header: ID\thomeaddress\tuni\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "file = open(\"customer.txt.txt\",\"r\");\n",
    "file = file.readlines();\n",
    "import csv\n",
    "\n",
    "def find_delimiter(filename):\n",
    "    sniffer = csv.Sniffer()\n",
    "    with open(filename) as fp:\n",
    "        delimiter = sniffer.sniff(fp.read(5000)).delimiter\n",
    "    if \"\\t\" in delimiter:\n",
    "        delimiter = \"\\\\t\"\n",
    "    return delimiter\n",
    "\n",
    "def detect_row_separator(file):\n",
    "    possible_separators = [\"\\n\", \"\\r\\n\"]\n",
    "    for text in file:\n",
    "        if \"\\r\\n\" in text:\n",
    "            print()\n",
    "            return \"\\\\r\\\\n\"\n",
    "        else: \n",
    "            \n",
    "            return \"\\\\n\"\n",
    "print(file)\n",
    "\n",
    "delimiter = find_delimiter(\"customer.txt.txt\")\n",
    "print(delimiter)\n",
    "row_separator = detect_row_separator(file)\n",
    "print(row_separator)\n",
    "head =  open(\"customer.txt.txt\",\"r\")\n",
    "\n",
    "Line_1 = \"Header: \" +head.readlines()[0].replace(delimiter, \",\")\n",
    "print(Line_1)\n",
    "Line_2 = \"Delimiter used in the text: \" +  str(delimiter)\n",
    "\n",
    "Line_3 = \"Row separator used in the text: \" + row_separator\n",
    "\n",
    "Line_4 = \"Number of records: \" + str(len(file)-1)\n",
    "\n",
    "lines = [Line_1, Line_2, Line_3, Line_4]\n",
    "\n",
    "with open(\"Detect_delimiter.txt\", \"w\") as f:\n",
    "    for line in lines:\n",
    "        f.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "819cce72-e144-4f0c-819a-537b3133b42c",
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Could not determine delimiter",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mError\u001b[0m                                     Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path) \u001b[38;5;28;01mas\u001b[39;00m csvfile:\n\u001b[1;32m---> 24\u001b[0m         dialect \u001b[38;5;241m=\u001b[39m \u001b[43mcsv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSniffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msniff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsvfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m         headers \u001b[38;5;241m=\u001b[39m csvfile\u001b[38;5;241m.\u001b[39mreadline()\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(dialect\u001b[38;5;241m.\u001b[39mdelimiter)\n\u001b[0;32m     26\u001b[0m         out\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFile: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\csv.py:187\u001b[0m, in \u001b[0;36mSniffer.sniff\u001b[1;34m(self, sample, delimiters)\u001b[0m\n\u001b[0;32m    183\u001b[0m     delimiter, skipinitialspace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_guess_delimiter(sample,\n\u001b[0;32m    184\u001b[0m                                                         delimiters)\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m delimiter:\n\u001b[1;32m--> 187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Error(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not determine delimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mdialect\u001b[39;00m(Dialect):\n\u001b[0;32m    190\u001b[0m     _name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msniffed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mError\u001b[0m: Could not determine delimiter"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "# Directory containing the files\n",
    "directory = 'C:/Users/dongv/Documents/GitHub/ReadfileTraining'\n",
    "\n",
    "# Output file\n",
    "output_file = 'output.txt'\n",
    "\n",
    "# Get a list of all files in the directory\n",
    "files = os.listdir(directory)\n",
    "\n",
    "# Open the output file for writing\n",
    "with open(output_file, 'w') as out:\n",
    "    # Write the number of files in the directory to the output file\n",
    "    out.write(f'Number of files in {directory}: {len(files)}\\n\\n')\n",
    "    # Iterate through each file\n",
    "    for file in files:\n",
    "        # Construct the full path to the file\n",
    "        file_path = os.path.join(directory, file)\n",
    "        # Check if the file is a CSV file\n",
    "        if file.endswith('.csv'):\n",
    "            with open(file_path) as csvfile:\n",
    "                dialect = csv.Sniffer().sniff(csvfile.read(1024))\n",
    "                headers = csvfile.readline().strip().split(dialect.delimiter)\n",
    "                out.write(f'File: {file}\\n')\n",
    "                out.write(f'Headers: {headers}\\n')\n",
    "                out.write(f'Delimiter: {dialect.delimiter}\\n')\n",
    "                #out.write(f'Row Separator: {dialect.linetermin}\\n\\n')\n",
    "        else:\n",
    "            out.write(f'{file} is not a csv file\\n\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f641ea90-e7c6-495c-8e0f-49d17c4536fd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Input directory\n",
    "input_directory = 'C:/Users/dongv/Documents/GitHub/ReadfileTraining'\n",
    "\n",
    "# Output directory\n",
    "output_directory = 'C:/Users/dongv/Documents/GitHub/ReadfileTraining'\n",
    "\n",
    "# Iterate over all files in the input directory\n",
    "for file_name in os.listdir(input_directory):\n",
    "    # Check if the file is a text file\n",
    "    if file_name.endswith('.txt'):\n",
    "        # Construct the full file path\n",
    "        input_file = os.path.join(input_directory, file_name)\n",
    "        output_file = os.path.join(output_directory, file_name[:-4] + '.csv')\n",
    "        # read your text file using pandas\n",
    "        data = pd.read_csv(input_file, sep = '\\t')\n",
    "        # write the data to csv\n",
    "        data.to_csv(output_file, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed7b016f-69a8-43e2-b5b5-8e3d2d93fe17",
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m delimiter \u001b[38;5;241m=\u001b[39m dialect\u001b[38;5;241m.\u001b[39mdelimiter\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#quotechar = dialect.quotechar\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Inspect header\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m header \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcsv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m out\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFile: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     18\u001b[0m out\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDelimiter: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdelimiter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "directory = 'C:/Users/dongv/Documents/GitHub/ReadfileTraining'\n",
    "output_file = 'C:/Users/dongv/Documents/GitHub/ReadfileTraining/output.txt'\n",
    "\n",
    "with open(output_file, 'w') as out:\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith('.csv'):\n",
    "            with open(os.path.join(directory, file), 'r') as f:\n",
    "                # Use csv.Sniffer to inspect delimiter and separator\n",
    "                dialect = csv.Sniffer().sniff(f.read(1024))\n",
    "                delimiter = dialect.delimiter\n",
    "                #quotechar = dialect.quotechar\n",
    "                # Inspect header\n",
    "                header = next(csv.reader(f, dialect))\n",
    "                out.write(f'File: {file}\\n')\n",
    "                out.write(f'Delimiter: {delimiter}\\n')\n",
    "                #out.write(f'Quotechar: {quotechar}\\n')\n",
    "                out.write(f'Header: {header}\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7676cde9-3ae1-4fd4-961d-fb009d43bde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Local Disk\n",
      " Volume Serial Number is ACB9-FBFA\n",
      "\n",
      " Directory of C:\\Users\\dongv\\Documents\\GitHub\\ReadfileTraining\n",
      "\n",
      "29/01/2023  03:53 PM    <DIR>          .\n",
      "27/01/2023  11:41 AM    <DIR>          ..\n",
      "27/01/2023  01:01 PM    <DIR>          .ipynb_checkpoints\n",
      "29/01/2023  03:31 PM                50 customer.txt - Copy (3).csv\n",
      "27/01/2023  05:35 PM                48 customer.txt - Copy (3).txt\n",
      "29/01/2023  03:31 PM                50 customer.txt - Copy (4).csv\n",
      "27/01/2023  05:35 PM                48 customer.txt - Copy (4).txt\n",
      "29/01/2023  03:31 PM                50 customer.txt - Copy (5).csv\n",
      "27/01/2023  05:35 PM                48 customer.txt - Copy (5).txt\n",
      "29/01/2023  03:31 PM                50 customer.txt.csv\n",
      "27/01/2023  05:35 PM                48 customer.txt.txt\n",
      "29/01/2023  03:31 PM                50 customer_day1.csv\n",
      "27/01/2023  05:35 PM                48 customer_day1.txt\n",
      "29/01/2023  03:31 PM                50 customer_day2.csv\n",
      "27/01/2023  05:35 PM                48 customer_day2.txt\n",
      "29/01/2023  03:31 PM                50 customer_day3.csv\n",
      "27/01/2023  05:35 PM                48 customer_day3.txt\n",
      "29/01/2023  03:31 PM                50 customer_day4.csv\n",
      "27/01/2023  05:35 PM                48 customer_day4.txt\n",
      "29/01/2023  03:31 PM                50 customer_day5.csv\n",
      "27/01/2023  05:35 PM                48 customer_day5.txt\n",
      "29/01/2023  03:31 PM               128 Detect_delimiter.csv\n",
      "29/01/2023  03:32 PM               120 Detect_delimiter.txt\n",
      "27/01/2023  05:54 PM    <DIR>          Filedelimited\n",
      "29/01/2023  03:53 PM            10,369 Fileread.ipynb\n",
      "16/10/2022  09:30 AM         2,326,072 KPMG_VI_New_raw_data_update_final1.xlsx\n",
      "29/01/2023  03:31 PM               659 output.csv\n",
      "29/01/2023  03:53 PM               179 output.txt\n",
      "29/01/2023  03:36 PM                 0 output1.txt\n",
      "              25 File(s)      2,338,409 bytes\n",
      "               4 Dir(s)  284,633,776,128 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8570d1a-2e57-40a0-99db-165befd9c03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Local Disk\n",
      " Volume Serial Number is ACB9-FBFA\n",
      "\n",
      " Directory of C:\\Users\\dongv\\Documents\\GitHub\\ReadfileTraining\n",
      "\n",
      "29/01/2023  03:57 PM    <DIR>          .\n",
      "27/01/2023  11:41 AM    <DIR>          ..\n",
      "27/01/2023  01:01 PM    <DIR>          .ipynb_checkpoints\n",
      "27/01/2023  05:35 PM                48 customer.txt.txt\n",
      "27/01/2023  05:54 PM    <DIR>          Filedelimited\n",
      "29/01/2023  03:57 PM            13,647 Fileread.ipynb\n",
      "16/10/2022  09:30 AM         2,326,072 KPMG_VI_New_raw_data_update_final1.xlsx\n",
      "               3 File(s)      2,339,767 bytes\n",
      "               4 Dir(s)  286,232,559,616 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b06c0806-9195-4b76-88ab-2425553f1564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Local Disk\n",
      " Volume Serial Number is ACB9-FBFA\n",
      "\n",
      " Directory of C:\\Users\\dongv\\Documents\\GitHub\\ReadfileTraining\n",
      "\n",
      "30/01/2023  09:17 AM    <DIR>          .\n",
      "27/01/2023  11:41 AM    <DIR>          ..\n",
      "27/01/2023  01:01 PM    <DIR>          .ipynb_checkpoints\n",
      "14/10/2022  11:29 PM           157,543 Content (1).csv\n",
      "27/01/2023  05:35 PM                48 customer.txt.txt\n",
      "27/01/2023  05:54 PM    <DIR>          Filedelimited\n",
      "30/01/2023  09:17 AM            14,685 Fileread.ipynb\n",
      "16/10/2022  09:30 AM         2,326,072 KPMG_VI_New_raw_data_update_final1.xlsx\n",
      "14/10/2022  11:29 PM            44,985 Location (1).csv\n",
      "14/10/2022  11:29 PM        19,206,075 potato.csv\n",
      "14/10/2022  11:29 PM            38,557 Profile (1).csv\n",
      "14/10/2022  11:29 PM        29,019,945 QVI_data.csv\n",
      "14/10/2022  11:29 PM         2,452,463 QVI_purchase_behaviour.csv\n",
      "14/10/2022  11:29 PM        17,823,115 QVI_transaction_data.csv\n",
      "14/10/2022  11:29 PM         2,650,513 Reactions (1).csv\n",
      "14/10/2022  11:29 PM               388 ReactionTypes (1).csv\n",
      "01/12/2022  12:41 PM           530,782 sales_data_sample.csv\n",
      "14/10/2022  11:29 PM            26,019 Session (1).csv\n",
      "14/10/2022  11:29 PM            34,327 User (1).csv\n",
      "              15 File(s)     74,325,517 bytes\n",
      "               4 Dir(s)  286,154,997,760 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "797ed8f9-a881-4a59-817e-8936b1956259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content (1).csv\n",
      "Location (1).csv\n",
      "potato.csv\n",
      "Profile (1).csv\n",
      "QVI_data.csv\n",
      "QVI_purchase_behaviour.csv\n",
      "QVI_transaction_data.csv\n",
      "Reactions (1).csv\n",
      "ReactionTypes (1).csv\n",
      "Session (1).csv\n",
      "User (1).csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "folder = 'C:/Users/dongv/Documents/GitHub/ReadfileTraining'\n",
    "files = [f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))]\n",
    "\n",
    "files.sort()\n",
    "\n",
    "# Directory containing the files\n",
    "directory = 'C:/Users/dongv/Documents/GitHub/ReadfileTraining'\n",
    "\n",
    "# Output file\n",
    "output_file = 'output.txt'\n",
    "\n",
    "# Get a list of all files in the directory\n",
    "files = os.listdir(directory)\n",
    "\n",
    "# Open the output file for writing\n",
    "with open(output_file, 'w') as out:\n",
    "    # Write the number of files in the directory to the output file\n",
    "    out.write(f'Number of files in {directory}: {len(files)}\\n\\n')\n",
    "    # Iterate through each file\n",
    "    for file in files:\n",
    "        # Construct the full path to the file\n",
    "        file_path = os.path.join(directory, file)\n",
    "        # Check if the file is a CSV file\n",
    "        if file.endswith('.csv'):\n",
    "            with open(file_path) as csvfile:\n",
    "                #print(csvfile.read(500))\n",
    "                print(file)\n",
    "                headers = csvfile.readline().strip().split(dialect.delimiter)\n",
    "                dialect = csv.Sniffer().sniff(csvfile.read())\n",
    "                row_separator = dialect.lineterminator.replace(\"\\n\", \"\\\\n\")\n",
    "                out.write(f'File: {file}\\n')\n",
    "                out.write(f'Headers: {headers}\\n')\n",
    "                out.write(f'Delimiter: {dialect.delimiter}\\n')\n",
    "                out.write(\"Row Separator: \" + row_separator + \"\\n\")\n",
    "                content = pd.read_csv(file, delimiter=dialect.delimiter)\n",
    "                if content.columns[0] == \"Unnamed: 0\" : \n",
    "                    content = pd.read_csv(file, delimiter=dialect.delimiter, index_col=0)\n",
    "                out.write(\"\\nNumber of records in file \" + file + \" is \"+ str(content.shape[0]))\n",
    "                out.write(\"\\nNumber of fields in file \" + file + \" is \" + str(content.shape[1]) + \"\\n\\n\")\n",
    "                for i in content.columns:\n",
    "                    out.write(\"Number of values in \" + i + \" column is \"  + str((~content[i].isnull()).sum())  + \"\\n\")\n",
    "                    out.write(\"Number of unique values in \" + i + \" column is \" + str(len(content[i].unique())) + \"\\n\")\n",
    "                    out.write(\"Number of null values in \" + i + \" column is \" + str((content[i].isnull()).sum())  + \"\\n\\n\")\n",
    "        else:\n",
    "            out.write(f'{file} is not a csv file\\n\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "479099ae-007b-4fb3-a7f3-169f474ceca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'Content ID', 'User ID', 'Type', 'Category', 'URL']\n",
      "0,97522e57-d9ab-4bd6-97bf-c24d952602d2,8d3cd87d-8a31-4935-9a4f-b319bfe05f31,photo,Studying,https://socialbuzz.cdn.com/content/storage/97522e57-d9ab-4bd6-97bf-c24d952602d2\n",
      "\n",
      ",\n",
      "\\n\n",
      "['1', '9f737e0a-3cdd-4d29-9d24-753f4e3be810', 'beb1f34e-7870-46d6-9fc7-2e12eb83ce43', 'photo', 'healthy eating', 'https://socialbuzz.cdn.com/content/storage/9f737e0a-3cdd-4d29-9d24-753f4e3be810']\n",
      "2,230c4e4d-70c3-461d-b42c-ec09396efb3f,a5c65404-5894-4b87-82f2-d787cbee86b4,photo,healthy eating,https://socialbuzz.cdn.com/content/storage/230c4e4d-70c3-461d-b42c-ec09396efb3f\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1000 entries, 0 to 999\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Content ID  1000 non-null   object\n",
      " 1   User ID     1000 non-null   object\n",
      " 2   Type        1000 non-null   object\n",
      " 3   Category    1000 non-null   object\n",
      " 4   URL         801 non-null    object\n",
      "dtypes: object(5)\n",
      "memory usage: 46.9+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "with open('Content (1).csv') as csvfile:\n",
    "    print(csvfile.readline().strip().split(dialect.delimiter))\n",
    "    file = csvfile.readline()\n",
    "    print(file)\n",
    "    dialect = csv.Sniffer().sniff(file)\n",
    "    print(dialect.delimiter)\n",
    "    print(dialect.lineterminator.replace(\"\\n\",\"\\\\n\"))\n",
    "    print(csvfile.readline().strip().split(dialect.delimiter))\n",
    "    print(csvfile.readline())\n",
    "    content = pd.read_csv(\"Content (1).csv\", delimiter=dialect.delimiter, index_col=0)\n",
    "    print(content.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "a533b035-865b-4cb3-997a-df6852d58729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Type', 'Sentiment', 'Score'], dtype='object')"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "bbb5327c-3de6-44a8-9dd2-6ffb446f5c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from html import escape\n",
    "\n",
    "def create_table():\n",
    "    table_data = [\n",
    "        ('Name', 'Age', 'City'),\n",
    "        ('John', 30, 'New York'),\n",
    "        ('Jane', 25, 'London')\n",
    "    ]\n",
    "\n",
    "    html = \"<table>\"\n",
    "    for row in table_data:\n",
    "        html += \"<tr>\"\n",
    "        for cell in row:\n",
    "            html += f\"<td style='border: 1px solid black;'>\" \\\n",
    "                    f\"{escape(str(cell))}</td>\"\n",
    "        html += \"</tr>\"\n",
    "    html += \"</table>\"\n",
    "    \n",
    "    return html\n",
    "\n",
    "def export_table(html_table):\n",
    "    with open(\"table.html\", \"w\") as f:\n",
    "        f.write(html_table)\n",
    "\n",
    "html_table = create_table()\n",
    "export_table(html_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d6a78b-9a7b-4523-8107-3aa6e9541f82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
